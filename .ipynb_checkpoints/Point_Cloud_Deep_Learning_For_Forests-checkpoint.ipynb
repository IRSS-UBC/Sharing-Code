{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Point Cloud Deep Learing With Forest Lidar Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Harry Seely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple tutorial to get you started with using a point cloud deep learning model to process forest lidar data.\n",
    "\n",
    "While deep learning is a but tough to use at first, once you get the hang of it you can begin to appreciate the flexibility and power of these models.\n",
    "\n",
    "If you would like more information on point cloud deep learning, check out my repo [https://github.com/harryseely/DL_Biomass](https://github.com/harryseely/DL_Biomass)\n",
    "\n",
    "\n",
    "This tutorial was developed using python v3.8\n",
    "\n",
    "All you need to run it is a python environment setup with jupyter notebook installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Install requirements\n",
    "# !pip install torch torchvision torchaudio\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load required modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Point Cloud Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So lets start with some simple point clouds. How about 12 forest plots point clouds each which has 7168 points or 18 pts/m^2  (to keep file size small). It is required for most neural networks that each input file has the same dimensions, which is why the point clouds need to be pre-processed to have the same number of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get our files\n",
    "in_dir = r\"data\"\n",
    "glob = \"*.npy\"\n",
    "\n",
    "#Read in all files\n",
    "files = list(Path(in_dir).glob(glob))\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have three datasets: train, validation, and test\n",
    "\n",
    "Lets visualize one of these point clouds.\n",
    "\n",
    "Each point cloud has 8 columns and 7168 rows. The columns include xyz and other lidar attributes such as intensity and scan angle. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load point cloud from numpy file\n",
    "points = np.load(\"data/NBGOV10160_7168_test_duplicate_or_fps.npy\")\n",
    "\n",
    "# Extract xyz only\n",
    "xyz = points[:, 0:3]\n",
    "\n",
    "# set up a figure twice as wide as it is tall\n",
    "fig = plt.figure(figsize=[30, 30])\n",
    "\n",
    "# set up the axes for the first plot\n",
    "ax = fig.add_subplot(1, 1, 1, projection='3d')\n",
    "ax.scatter(xyz[:, 0], xyz[:, 1], xyz[:, 2], c=xyz[:, 2], linewidth=1, s=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so its a bit hard to see in this figure but this is a 11.28m (399.7m2) forest plot\n",
    "\n",
    "For a simple initial modelling task, we can get the model to predict the average height for each plot. Lets calculate that for every plot in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For example, this plot has a mean height of:\n",
    "np.mean(points[:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to develop to get a pytorch DL model up and running is a dataset class. This is used to load data into our model during training. We can define a simple one below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 12\n",
      "torch.Size([1])\n",
      "\n",
      "Sample info:\n",
      "\n",
      "Type:  <class 'dict'>\n",
      "Shape:  2\n",
      "Points shape:  (7168, 3)\n",
      "Target:  tensor([2.8866e-15])\n"
     ]
    }
   ],
   "source": [
    "class PointCloudDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, glob=\"*.npy\"):\n",
    "        #Read in all files and attach to the class\n",
    "        self.files = list(Path(in_dir).glob(glob))        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        points = np.load(self.files[idx])\n",
    "        \n",
    "        #Subset to xyz cols only\n",
    "        points = points[:, :3]\n",
    "        \n",
    "        # Centralize coordinates\n",
    "        points[:, 0:3] = points[:, 0:3] - np.mean(points[:, 0:3], axis=0)\n",
    "        \n",
    "        #Get the mean height\n",
    "        target = np.mean(points[:,2])\n",
    "        \n",
    "        #Make correct shape to be tensor\n",
    "        target = np.reshape(target, (1,1))[0]\n",
    "        \n",
    "        #Input data needs to be in tensor format\n",
    "        torch.from_numpy(points).float().permute(1, 0)\n",
    "        target = torch.tensor(target, dtype=torch.float64).float()\n",
    "        \n",
    "        print(target.shape)\n",
    "        \n",
    "        sample = {'points': points, 'target': target}\n",
    "\n",
    "        return sample\n",
    "        \n",
    "#Lets test the Dataset class\n",
    "dataset = PointCloudDataset(data_dir=\"data\", glob=\"*.npy\")\n",
    "\n",
    "#Check length\n",
    "print(\"Number of samples:\", len(dataset))\n",
    "\n",
    "#Check one sample\n",
    "data_iter = iter(dataset)\n",
    "sample = next(data_iter)\n",
    "\n",
    "print(\"\\nSample info:\\n\")\n",
    "print(\"Type: \", type(sample))\n",
    "print(\"Shape: \", len(sample))\n",
    "print(\"Points shape: \", sample[\"points\"].shape)\n",
    "print(\"Target: \", sample['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a class to load the data into the model in batches. A batch is a subset of data that passes through the model before the loss is calculated and the optimizer is implemented. For simplicity, we will use a batch size of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "Batch shape explained:\n",
      "Batch size, Num points, XYZ coords\n",
      "torch.Size([2, 7168, 3])\n",
      "Target:\n",
      "tensor([[9.4528e-15],\n",
      "        [3.8065e-16]])\n"
     ]
    }
   ],
   "source": [
    "#Set up data loader (loads data into the model during each iteration)\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "#Test the loader\n",
    "loader_iter = iter(loader)\n",
    "batch = next(loader_iter)\n",
    "print(\"Batch shape explained:\")\n",
    "print(\"Batch size, Num points, XYZ coords\")\n",
    "print(batch['points'].shape)\n",
    "print(f\"Target:\\n{batch['target']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I asked ChatGPT to write me some code for a simple point cloud neural network and here it was it provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DGCNN(\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(6, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (conv4): Sequential(\n",
      "    (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (conv5): Sequential(\n",
      "    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (linear1): Linear(in_features=2048, out_features=512, bias=False)\n",
      "  (bn6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dp1): Dropout(p=0.5, inplace=False)\n",
      "  (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (bn7): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dp2): Dropout(p=0.5, inplace=False)\n",
      "  (linear3): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Pytorch Implementation: https://github.com/WangYueFt/dgcnn\n",
    "#Citation: Wang, Yue, et al. \"Dynamic graph cnn for learning on point clouds.\" Acm Transactions On Graphics (tog) 38.5 (2019): 1-12.\n",
    "def knn(x, k):\n",
    "    inner = -2 * torch.matmul(x.transpose(2, 1), x)\n",
    "    xx = torch.sum(x ** 2, dim=1, keepdim=True)\n",
    "    pairwise_distance = -xx - inner - xx.transpose(2, 1)\n",
    "\n",
    "    idx = pairwise_distance.topk(k=k, dim=-1)[1]  # (batch_size, num_points, k)\n",
    "    return idx\n",
    "\n",
    "\n",
    "def get_graph_feature(x, k=20, idx=None):\n",
    "    batch_size = x.size(0)\n",
    "    num_points = x.size(2)\n",
    "    x = x.view(batch_size, -1, num_points)\n",
    "    if idx is None:\n",
    "        idx = knn(x, k=k)  # (batch_size, num_points, k)\n",
    "    device = x.device #Changed this from original torch.device('cuda') to try and get it to work on DDP\n",
    "\n",
    "    idx_base = torch.arange(0, batch_size, device=device).view(-1, 1, 1) * num_points\n",
    "\n",
    "    idx = idx + idx_base\n",
    "\n",
    "    idx = idx.view(-1)\n",
    "\n",
    "    _, num_dims, _ = x.size()\n",
    "\n",
    "    x = x.transpose(2,\n",
    "                    1).contiguous()  # (batch_size, num_points, num_dims)  -> (batch_size*num_points, num_dims) #   batch_size * num_points * k + range(0, batch_size*num_points)\n",
    "    feature = x.view(batch_size * num_points, -1)[idx, :]\n",
    "    feature = feature.view(batch_size, num_points, k, num_dims)\n",
    "    x = x.view(batch_size, num_points, 1, num_dims).repeat(1, 1, k, 1)\n",
    "\n",
    "    feature = torch.cat((feature - x, x), dim=3).permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "    return feature\n",
    "\n",
    "\n",
    "class DGCNN(nn.Module):\n",
    "    def __init__(self, dropout=0.5, k=20, emb_dims=1024, n_outs=1):\n",
    "        super(DGCNN, self).__init__()\n",
    "        \n",
    "        #Network config arguments\n",
    "        self.k = k\n",
    "        self.dropout = dropout\n",
    "        self.emb_dims = emb_dims\n",
    "\n",
    "        #Start of network\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.bn5 = nn.BatchNorm1d(self.emb_dims)\n",
    "\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(6, 64, kernel_size=1, bias=False),\n",
    "                                   self.bn1,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(64 * 2, 64, kernel_size=1, bias=False),\n",
    "                                   self.bn2,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(64 * 2, 128, kernel_size=1, bias=False),\n",
    "                                   self.bn3,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv4 = nn.Sequential(nn.Conv2d(128 * 2, 256, kernel_size=1, bias=False),\n",
    "                                   self.bn4,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv5 = nn.Sequential(nn.Conv1d(512, self.emb_dims, kernel_size=1, bias=False),\n",
    "                                   self.bn5,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.linear1 = nn.Linear(self.emb_dims * 2, 512, bias=False)\n",
    "        self.bn6 = nn.BatchNorm1d(512)\n",
    "        self.dp1 = nn.Dropout(p=self.dropout)\n",
    "        self.linear2 = nn.Linear(512, 256)\n",
    "        self.bn7 = nn.BatchNorm1d(256)\n",
    "        self.dp2 = nn.Dropout(p=self.dropout)\n",
    "        self.linear3 = nn.Linear(256, n_outs)\n",
    "\n",
    "    def forward(self, points):\n",
    "\n",
    "        #For now, only use coordinates for DGCNN\n",
    "        xyz = points[:, :2, :]\n",
    "\n",
    "        batch_size = xyz.size(0)\n",
    "        x = get_graph_feature(xyz, k=self.k)\n",
    "        x = self.conv1(x)\n",
    "        x1 = x.max(dim=-1, keepdim=False)[0]\n",
    "\n",
    "        x = get_graph_feature(x1, k=self.k)\n",
    "        x = self.conv2(x)\n",
    "        x2 = x.max(dim=-1, keepdim=False)[0]\n",
    "\n",
    "        x = get_graph_feature(x2, k=self.k)\n",
    "        x = self.conv3(x)\n",
    "        x3 = x.max(dim=-1, keepdim=False)[0]\n",
    "\n",
    "        x = get_graph_feature(x3, k=self.k)\n",
    "        x = self.conv4(x)\n",
    "        x4 = x.max(dim=-1, keepdim=False)[0]\n",
    "\n",
    "        x = torch.cat((x1, x2, x3, x4), dim=1)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x1 = F.adaptive_max_pool1d(x, 1).view(batch_size, -1)\n",
    "        x2 = F.adaptive_avg_pool1d(x, 1).view(batch_size, -1)\n",
    "        x = torch.cat((x1, x2), 1)\n",
    "\n",
    "        x = F.leaky_relu(self.bn6(self.linear1(x)), negative_slope=0.2)\n",
    "        x = self.dp1(x)\n",
    "        x = F.leaky_relu(self.bn7(self.linear2(x)), negative_slope=0.2)\n",
    "        x = self.dp2(x)\n",
    "        x = self.linear3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = DGCNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a VERY simple neural network that only has 2 layers, each with 64 neurons, for a total of 128 neurons.\n",
    "\n",
    "Let's test the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "selected index k out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[165], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpoints\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\pcdl_tutorial\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[162], line 84\u001b[0m, in \u001b[0;36mDGCNN.forward\u001b[1;34m(self, points)\u001b[0m\n\u001b[0;32m     81\u001b[0m xyz \u001b[38;5;241m=\u001b[39m points[:, :\u001b[38;5;241m2\u001b[39m, :]\n\u001b[0;32m     83\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m xyz\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 84\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mget_graph_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxyz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[0;32m     86\u001b[0m x1 \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[1;32mIn[162], line 17\u001b[0m, in \u001b[0;36mget_graph_feature\u001b[1;34m(x, k, idx)\u001b[0m\n\u001b[0;32m     15\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, num_points)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 17\u001b[0m     idx \u001b[38;5;241m=\u001b[39m \u001b[43mknn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (batch_size, num_points, k)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m device \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;66;03m#Changed this from original torch.device('cuda') to try and get it to work on DDP\u001b[39;00m\n\u001b[0;32m     20\u001b[0m idx_base \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, batch_size, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m num_points\n",
      "Cell \u001b[1;32mIn[162], line 8\u001b[0m, in \u001b[0;36mknn\u001b[1;34m(x, k)\u001b[0m\n\u001b[0;32m      5\u001b[0m xx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(x \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m pairwise_distance \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mxx \u001b[38;5;241m-\u001b[39m inner \u001b[38;5;241m-\u001b[39m xx\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[43mpairwise_distance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# (batch_size, num_points, k)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m idx\n",
      "\u001b[1;31mRuntimeError\u001b[0m: selected index k out of range"
     ]
    }
   ],
   "source": [
    "model(batch['points'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function is how the way you measure model accuracy. In this case since it is a regression problem, let use Mean Squared Error (MSE). \n",
    "\n",
    "The optimizer is a function that uses complex calculus and linear algebra to adjust the model neurons after each training iteration based on the loss. For example, if the loss is very large, the optimizer might make big changes to the network parameters, if it is small, then the opposite may occur.\n",
    "\n",
    "The learning rate (lr) determines how fast/slow our optimizer helps our model learn. Larger learning rate = faster, but more chaotic learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up a loss function and an optimizer.\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning models are trained by iteratively passing the dataset through the network and then adjusting the neuron parameters (i.e., weights) based on each iteration. \n",
    "\n",
    "Once the entire training dataset has moved through the model, this is considered to be 1 epoch. \n",
    "\n",
    "Deep learning models are often trained for 100s or 1000s of epochs.\n",
    "\n",
    "The dataloader will load one or several samples during each iteration within an epoch. The number of samples loaded during an iteration is determined by the batch size. Here we will use a batch size of 1. Batch size can be larger though. For example, ChatGPT was trained using a batch size of 2 Million pieces of text.\n",
    "\n",
    "Once the batch is loaded, it is processed through the model, the model makes a prediction, the loss (i.e., error) is calculated, and then the optimizer is used to adjust the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(100):\n",
    "    for step, batch in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(batch['points'])\n",
    "        loss = loss_fn(pred, batch['target'])\n",
    "        print(pred)\n",
    "        print(batch['target'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, 100, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
